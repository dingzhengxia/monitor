# --- START OF FILE app/services/data_fetcher.py (WITH IGNORE_FILTERS FLAG) ---
import time
import pandas as pd
from loguru import logger
from collections import defaultdict


def get_top_n_symbols_by_volume(exchange, top_n=100, exclude_list=[], market_type='swap', retries=5, config=None,
                                ignore_adv_filters=False):
    # æ­¥éª¤ 0: è§£æé…ç½®ï¼Œå¹¶æä¾›å®‰å…¨çš„é»˜è®¤å€¼
    scan_conf = config.get('market_settings', {}).get('dynamic_scan', {}) if config else {}
    primary_quote = scan_conf.get('primary_quote_currency', 'USDT').upper()

    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘: æ ¹æ® ignore_adv_filters æ ‡å¿—å†³å®šæ˜¯å¦å¯ç”¨é«˜çº§ç­›é€‰
    cross_filter_enabled = False
    if not ignore_adv_filters:
        cross_filter_conf = scan_conf.get('cross_market_filter', {})
        cross_filter_enabled = cross_filter_conf.get('enabled', False)

    must_exist_quotes = set([q.upper() for q in scan_conf.get('cross_market_filter', {}).get('must_exist_in', [])])

    logger.info(f"...æ­£åœ¨ä» {exchange.id} è·å–æ‰€æœ‰äº¤æ˜“å¯¹çš„24hè¡Œæƒ…æ•°æ® (ç›®æ ‡å¸‚åœº: {market_type})...")
    logger.info(f"ä¸»è®¡ä»·è´§å¸: {primary_quote}")

    if cross_filter_enabled and not ignore_adv_filters:
        if must_exist_quotes:
            logger.info(f"ğŸ¯ è·¨å¸‚åœºéªŒè¯å·²æ¿€æ´»ã€‚å¸ç§å¿…é¡»åŒæ—¶å­˜åœ¨äº: {primary_quote} AND {', '.join(must_exist_quotes)}")
        else:
            logger.warning("âš ï¸ è·¨å¸‚åœºéªŒè¯å·²å¯ç”¨ï¼Œä½† 'must_exist_in' åˆ—è¡¨ä¸ºç©ºã€‚å°†åªæ‰«æä¸»å¸‚åœºã€‚")
    else:
        if ignore_adv_filters:
            logger.info(f"å¸¸è§„åŠ¨æ€æ‰«ææ¨¡å¼ (å·²å¿½ç•¥é«˜çº§ç­›é€‰)ã€‚")
        else:
            logger.info(f"å¸¸è§„åŠ¨æ€æ‰«ææ¨¡å¼ã€‚")

    for i in range(retries):
        try:
            tickers = exchange.fetch_tickers()
            logger.info(f"...è·å–æˆåŠŸï¼Œå…± {len(tickers)} ä¸ªtickerï¼Œæ­£åœ¨å¤„ç†...")

            # ... (åç»­ä»£ç ä¸ä¹‹å‰ç‰ˆæœ¬å®Œå…¨ç›¸åŒ) ...

            # æ­¥éª¤ 1: æ„å»ºåœ°å›¾
            base_to_quotes_map = defaultdict(set)
            primary_market_tickers = {}

            for symbol_str, ticker in tickers.items():
                if not ticker: continue

                symbol = ticker.get('symbol', symbol_str)
                is_swap = ticker.get('swap', False) or ':' in symbol
                if market_type == 'swap' and not is_swap: continue

                is_spot = ticker.get('spot', False) or '/' in symbol and not is_swap
                if market_type == 'spot' and not is_spot: continue

                base = ticker.get('base', symbol.split('/')[0].split(':')[0]).upper()
                quote = ticker.get('quote', symbol.split(':')[-1] if ':' in symbol else symbol.split('/')[-1]).upper()

                if base in exclude_list:
                    continue

                base_to_quotes_map[base].add(quote)

                if quote == primary_quote:
                    primary_market_tickers[base] = ticker

            # æ­¥éª¤ 2: ç­›é€‰åŠ¨æ€å€™é€‰å¸ç§
            candidate_bases = set()

            if cross_filter_enabled and must_exist_quotes and not ignore_adv_filters:
                required_quotes_for_check = must_exist_quotes.union({primary_quote})
                for base, existing_quotes in base_to_quotes_map.items():
                    if required_quotes_for_check.issubset(existing_quotes):
                        candidate_bases.add(base)
                logger.info(f"é€šè¿‡è·¨å¸‚åœºéªŒè¯çš„å¸ç§æœ‰ {len(candidate_bases)} ä¸ªã€‚")
            else:
                candidate_bases = set(primary_market_tickers.keys())

            # æ­¥éª¤ 3: æ’åºå¹¶è¿”å›åŠ¨æ€åˆ—è¡¨
            dynamic_candidates = []
            for base in candidate_bases:
                if base in primary_market_tickers:
                    ticker = primary_market_tickers[base]
                    if ticker.get('quoteVolume', 0) > 0:
                        dynamic_candidates.append({
                            'symbol': ticker['symbol'],
                            'volume': ticker['quoteVolume']
                        })

            sorted_tickers = sorted(dynamic_candidates, key=lambda x: x['volume'], reverse=True)[:top_n]
            logger.info(f"âœ… æˆåŠŸç­›é€‰å‡º {len(sorted_tickers)} ä¸ªåŠ¨æ€äº¤æ˜“å¯¹ã€‚")
            return [t['symbol'] for t in sorted_tickers]

        except Exception as e:
            logger.warning(f"è·å–è¡Œæƒ…æ•°æ®å¤±è´¥ (å°è¯• {i + 1}/{retries}): {e}")
            if i < retries - 1:
                time.sleep(exchange.rateLimit / 1000)
            else:
                logger.error(f"âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚");
                return []
    return []


def fetch_ohlcv_data(exchange, symbol, timeframe, limit):
    try:
        ohlcv = exchange.fetch_ohlcv(symbol, timeframe, limit=limit)
        if not ohlcv or len(ohlcv) < 50:
            return None
        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        return df
    except Exception as e:
        logger.debug(f"ä¸º {symbol} {timeframe} è·å–OHLCVæ•°æ®å¤±è´¥: {e}")
        return None