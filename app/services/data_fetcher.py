import time
import pandas as pd
import requests
import json
from loguru import logger
from collections import defaultdict


def fetch_fear_greed_index():
    """ ä» alternative.me API è·å–ææ…Œä¸è´ªå©ªæŒ‡æ•° """
    try:
        logger.info("...æ­£åœ¨è·å–ææ…Œè´ªå©ªæŒ‡æ•°...")
        response = requests.get("https://api.alternative.me/fng/?limit=1", timeout=10)
        response.raise_for_status()
        data = response.json()
        if 'data' in data and len(data['data']) > 0:
            latest_data = data['data'][0]
            logger.info(f"âœ… æˆåŠŸè·å–ææ…Œè´ªå©ªæŒ‡æ•°: {latest_data['value']} ({latest_data['value_classification']})")
            return {
                "value": latest_data['value'],
                "classification": latest_data['value_classification']
            }
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ è·å–ææ…Œè´ªå©ªæŒ‡æ•°æ—¶ç½‘ç»œé”™è¯¯: {e}")
    except json.JSONDecodeError:
        logger.error("âŒ è§£æææ…Œè´ªå©ªæŒ‡æ•°å“åº”å¤±è´¥ï¼Œä¸æ˜¯æœ‰æ•ˆçš„JSONã€‚")
    except Exception as e:
        logger.error(f"âŒ è·å–ææ…Œè´ªå©ªæŒ‡æ•°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
    return None


def fetch_funding_rate(exchange, symbol):
    """
    è·å–æŒ‡å®šäº¤æ˜“å¯¹çš„å½“å‰èµ„é‡‘è´¹ç‡
    """
    try:
        # å¤§å¤šæ•°äº¤æ˜“æ‰€ (Binance, OKX, Bybit) éƒ½æ”¯æŒæ­¤æ–¹æ³•
        funding_info = exchange.fetch_funding_rate(symbol)
        return funding_info
    except AttributeError:
        # å¦‚æœäº¤æ˜“æ‰€ä¸æ”¯æŒ fetch_funding_rate (è¾ƒå°‘è§)
        logger.debug(f"äº¤æ˜“æ‰€ä¸æ”¯æŒ fetch_funding_rate: {symbol}")
        return None
    except Exception as e:
        logger.debug(f"è·å–èµ„é‡‘è´¹ç‡å¤±è´¥ {symbol}: {e}")
        return None


def get_top_n_symbols_by_volume(exchange, top_n=100, exclude_list=[], market_type='swap', retries=5, config=None,
                                ignore_adv_filters=False):
    scan_conf = config.get('market_settings', {}).get('dynamic_scan', {}) if config else {}
    primary_quote = scan_conf.get('primary_quote_currency', 'USDT').upper()

    cross_filter_enabled = False
    if not ignore_adv_filters:
        cross_filter_conf = scan_conf.get('cross_market_filter', {})
        cross_filter_enabled = cross_filter_conf.get('enabled', False)

    must_exist_quotes = set([q.upper() for q in scan_conf.get('cross_market_filter', {}).get('must_exist_in', [])])

    logger.info(f"...æ­£åœ¨ä» {exchange.id} è·å–æ‰€æœ‰äº¤æ˜“å¯¹çš„24hè¡Œæƒ…æ•°æ® (ç›®æ ‡å¸‚åœº: {market_type})...")
    logger.info(f"ä¸»è®¡ä»·è´§å¸: {primary_quote}")

    if cross_filter_enabled and not ignore_adv_filters:
        if must_exist_quotes:
            logger.info(f"ğŸ¯ è·¨å¸‚åœºéªŒè¯å·²æ¿€æ´»ã€‚å¸ç§å¿…é¡»åŒæ—¶å­˜åœ¨äº: {primary_quote} AND {', '.join(must_exist_quotes)}")
        else:
            logger.warning("âš ï¸ è·¨å¸‚åœºéªŒè¯å·²å¯ç”¨ï¼Œä½† 'must_exist_in' åˆ—è¡¨ä¸ºç©ºã€‚å°†åªæ‰«æä¸»å¸‚åœºã€‚")
    else:
        if ignore_adv_filters:
            logger.info(f"å¸¸è§„åŠ¨æ€æ‰«ææ¨¡å¼ (å·²å¿½ç•¥é«˜çº§ç­›é€‰)ã€‚")
        else:
            logger.info(f"å¸¸è§„åŠ¨æ€æ‰«ææ¨¡å¼ã€‚")

    for i in range(retries):
        try:
            tickers = exchange.fetch_tickers()
            logger.info(f"...è·å–æˆåŠŸï¼Œå…± {len(tickers)} ä¸ªtickerï¼Œæ­£åœ¨å¤„ç†...")

            base_to_quotes_map = defaultdict(set)
            primary_market_tickers = {}

            for symbol_str, ticker in tickers.items():
                if not ticker: continue

                symbol = ticker.get('symbol', symbol_str)
                is_swap = ticker.get('swap', False) or ':' in symbol
                if market_type == 'swap' and not is_swap: continue

                is_spot = ticker.get('spot', False) or '/' in symbol and not is_swap
                if market_type == 'spot' and not is_spot: continue

                base = ticker.get('base', symbol.split('/')[0].split(':')[0]).upper()
                quote = ticker.get('quote', symbol.split(':')[-1] if ':' in symbol else symbol.split('/')[-1]).upper()

                if base in exclude_list:
                    continue

                base_to_quotes_map[base].add(quote)

                if quote == primary_quote:
                    primary_market_tickers[base] = ticker

            candidate_bases = set()

            if cross_filter_enabled and must_exist_quotes and not ignore_adv_filters:
                required_quotes_for_check = must_exist_quotes.union({primary_quote})
                for base, existing_quotes in base_to_quotes_map.items():
                    if required_quotes_for_check.issubset(existing_quotes):
                        candidate_bases.add(base)
                logger.info(f"é€šè¿‡è·¨å¸‚åœºéªŒè¯çš„å¸ç§æœ‰ {len(candidate_bases)} ä¸ªã€‚")
            else:
                candidate_bases = set(primary_market_tickers.keys())

            dynamic_candidates = []
            for base in candidate_bases:
                if base in primary_market_tickers:
                    ticker = primary_market_tickers[base]
                    if ticker.get('quoteVolume', 0) > 0:
                        dynamic_candidates.append({
                            'symbol': ticker['symbol'],
                            'volume': ticker['quoteVolume']
                        })

            sorted_tickers = sorted(dynamic_candidates, key=lambda x: x['volume'], reverse=True)[:top_n]
            logger.info(f"âœ… æˆåŠŸç­›é€‰å‡º {len(sorted_tickers)} ä¸ªåŠ¨æ€äº¤æ˜“å¯¹ã€‚")
            return [t['symbol'] for t in sorted_tickers]

        except Exception as e:
            logger.warning(f"è·å–è¡Œæƒ…æ•°æ®å¤±è´¥ (å°è¯• {i + 1}/{retries}): {e}")
            if i < retries - 1:
                time.sleep(exchange.rateLimit / 1000)
            else:
                logger.error(f"âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚");
                return []
    return []


def fetch_ohlcv_data(exchange, symbol, timeframe, limit):
    try:
        ohlcv = exchange.fetch_ohlcv(symbol, timeframe, limit=limit)
        if not ohlcv or len(ohlcv) < 50:
            return None
        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        return df
    except Exception as e:
        logger.debug(f"ä¸º {symbol} {timeframe} è·å–OHLCVæ•°æ®å¤±è´¥: {e}")
        return None